<!DOCTYPE HTML>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="author" content="co-authored by Chen Chen, Ethan Remsberg and Jason Tan">
<title>A Visual Tour to Empirical Neural Network Robustness</title>
<link rel="stylesheet" href="./style.css">
<link rel="stylesheet" href="./distill.css">
<script src="//polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="template.v1.js"></script>
<script src="//d3js.org/d3.v5.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/d3-legend/2.25.6/d3-legend.min.js"></script>
</head>
<body>
    <d-title>
        <h1>A Visual Tour to Empirical Neural Network Robustness</h1><br>   
    </d-title>
    <d-byline>
  <div class="byline grid">
    <div class="authors-affiliations grid">
      <h3>Authors</h3>
      <h3>Affiliations</h3>
      
        <p class="author">
          
            <a class="name" href="https://ccdtc.cc/">Chen Chen</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://www.cs.umd.edu/">University of Maryland</a>
        </p>
      
        <p class="author">
          
            <a class="name" href="https://github.com/ethan-rems">Ethan Remsberg</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://www.cs.umd.edu/">University of Maryland</a>
        </p>
      
        <p class="author">
          
            <a class="name" href="https://github.com/omivore">Jason Tan</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://www.cs.umd.edu/">University of Maryland</a>
        </p>
      
    </div>
    <div>
      <h3>Published</h3>
      
        <p><em>Not published yet.</em></p>
    </div>
    <div>
      <h3>DOI</h3>
      
        <p><em>No DOI yet.</em></p>
    </div>
  </div>
</d-byline>
    <h2> Neural Network's Success </h2>
    <p class="text4G"> Neural networks are the most powerful classifiers for image classification tasks at present. To obtain such a classifier, one first needs to define a network architecture \(f(w,x)\)
        parameterized by \(w\) as well as a loss function \(L(w, x, y)\) where \(y\) denotes the real labels, and then perform the gradient descent optimization over the loss function to obtain 
        a set of (sub)optimal parameters \(w\). <br>
        In the visualizations below, we show how the loss value and the test accuracy evolve, and how the output probabilities change for 10 example images using a benchmark neural network 
        ResNet-18<dt-cite key="he2016deep"></dt-cite> on the commonly-used dataset CIFAR-10<dt-cite key="krizhevsky2009learning"></dt-cite> (the training detail can be found in <dt-cite key="zeng2020adversarial"></dt-cite>) . Select an image, hover over the left-hand-side line chart, and then a racing bar chart 
        will display the changes of the predictive probabilities at different training epochs. You could find some interesting patterns there, for example, the prediction for the <b>Dog</b> 
        image (2nd row, 1st column) jump between <b>Cat</b> and <b>Dog</b> many times as the training goes.
    </p>
    <section id="section4G1">
        <div class="div4G" style="text-align:center;">
            <svg id="G1"></svg>
        </div>
    </section>
    <h2> PGD Attack </h2>
    <p class="text4G"> Consider the situation where an input image is poisoned by some pixel-level noise that is imperceptible to the human eye. In general, humans won’t be fooled; but unfortunately this is not 
        true for neural networks. In fact, noises found in certain ways are able to mislead a neural network almost always, and one of the popular attackers is <i>PGD attack</i> <dt-cite key="goodfellow2014explaining,madry2017towards"></dt-cite>. As its name
        indicates, PGD attack is a gradient-based attacker. It main contains the following steps:
        <ol>
            <li>Start from a random perturbation \(x^{\prime}\) of a given clean image \(x\) such that \(\|x^{\prime} - x\|_p \leq \epsilon\);</li>
            <li>Update \(x^{\prime}\) by one step of gradient decent to increase the loss value;</li>
            <li>Project \(x^{\prime}\) back into the \(\epsilon\)-radius \(L_p\) ball if necessary;</li>
            <li>Repeat setp 2–3 until convergence or reaching a desired number of steps \(ns\).</li>
        </ol>
    </p>
    <p class="text4G">
        Note that the \(\epsilon\)-radius \(L_p\) ball above is to enforce the condition that "<i>the noise is imperceptible to the human eye</i>". In the visualizations below, we set the hyperparameters
         as \(\{p=+\infty\), \(\epsilon=8/255\), \(ns=10\}\), and display how the prediction is manipulated and how the adversarial images look like at different steps of PGD attack. In most cases, the 
         attacker succeeds in no more than 3 steps. Do you have any other interesting findings?
    </p>
    <section id="section4G2">
        <div id="G2" class="div4G" style="text-align:center;">
            <div id="slider">
                <input id="epoch_slider">
                <p id="slider_text"></p>
            </div>
        </div>
    </section>
    <h2> Adversarial Training </h2>
    <p class="text4G">
        Here comes a natural question: can we avoid this? The answer is yes! An popular and effective solution is <i>adversarial training</i> <dt-cite key="madry2017towards"></dt-cite>. The idea is just to utilize the PGD attack in your 
        training process: training your model with adversarial images instead of clean images at each epoch. Although the idea is simple, the optimization behind is not trivial. In standard training, we
        solve the following minimization problem:
        $$
            \min _{\theta} \rho(\theta), \quad \text { where } \quad \rho(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[L(w, x, y)\right],
        $$
        while in adversarial training, we are solving a Min-Max problem:
        $$
            \min _{\theta} \rho(\theta), \quad \text { where } \quad \rho(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\max _{\delta \in \mathcal{S}} L(w, x+\delta, y)\right],
        $$
        where \(\mathcal{S} = \{x^{\prime}\mid\|x^{\prime} - x\|_p \leq \epsilon\}\) denotes the \(\epsilon\)-radius \(L_p\) ball. In theory, the convergence condition for such a Min-Max problem over a general
        function is hard to determine, while adversarial training converges for most neural networks in practice. In the following visualizations, we compare the output probabilities of a regular-trained model
        an adversarial-trained model when the same PGD attack is applied, and display how the probabilities flow as the PGD attack goes. It is clear that the adversarial-trained model is more robust. One thing
        worth mentioning here is that the adversarial-trained model cannot resist PGD attack all the time: its accuracy under the PGD attack is \(50.51\%\) (that for the regular-trained model is only \(0.01\%\)).
    </p>
    <section id="section4G3">
        <div id="G3" class="div4G" style="text-align:center;">
        </div>
    </section>
    <h2>
        Accuracy-Robustness Tradeoff
    </h2>
    <p class="text4G">
        There is no free lunch: adversarial training enhances the model's robustness, however, at the price of a standard accuracy drop. The adversarial-trained model achieves an \(80.05\%\) accuracy on
        clean test images while that for the regular-trained model is \(91.64\%\). This is a classic Accuracy-Robustness tradeoff, and people are now interested in how we can achieve both. The line chart below
        gives a sense about how this tradeoff looks like during the training.
    </p>
    <section id="section4G4">
        <div id="div4G4" class="div4G" style="text-align:center;">
            <svg id="G4"></svg>
        </div>
    </section>
    <p class="text4G">
        Other than a binary thing, adversarial training is influenced by the hyperparameter \(\epsilon\), thus it is expected that the Accuracy-Robustness tradeoff varies when \(\epsilon\) changes. In general,
        a smaller \(\epsilon\) leads to an adversarial-trained model that is more accurate on clean images but less resistant to PGD attack, and versa vice. We adversarial-trained 5 models with \(\epsilon=1/255,\ 2/255,\ 4/255,\ 8/266,\ 16/255\) 
        (other hyperparameters, as well as the attacker, are the same to make a fair comparison), and present their clean accuracy and adversarial accuracy (i.e., accuracy under PGD attack) in the scatter plot below.
        Note that the model with \(\epsilon=16/255\) doesn't bring a better robustness than that of the model with \(\epsilon=8/255\), thus the rule mentioned above is only valid within a certain range.
    </p>
    <section id="section4G5">
        <div id="div4G5" class="div4G" style="text-align:center;">
            <svg id="G5"></svg>
        </div>
    </section>
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <h3>
        Citations
    </h3>
    <p>
        [1] He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. <br>
        [2] Krizhevsky, Alex, and Geoffrey Hinton. "Learning multiple layers of features from tiny images." (2009): 7.<br>
        [3] Zeng, Huimin, et al. "Are Adversarial Examples Created Equal? A Learnable Weighted Minimax Risk for Robustness under Non-uniform Attacks." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.<br>
        [4] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. "Explaining and harnessing adversarial examples." arXiv preprint arXiv:1412.6572 (2014).<br>
        [5] Madry, Aleksander, et al. "Towards Deep Learning Models Resistant to Adversarial Attacks." International Conference on Learning Representations. 2018.<br>
    </p>

    
</body>
<script src="./d3-tip.js"></script>
<script src="./G1/main.js"></script>
<script src="./G2/main.js"></script>
<script src="./G3/main.js"></script>
<script src="./G4/main.js"></script>
<script src="./G5/main.js"></script>

<script type="text/bibliography">
    @inproceedings{he2016deep,
        title={Deep residual learning for image recognition},
        author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
        booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
        pages={770--778},
        year={2016}
      }
    @article{krizhevsky2009learning,
        title={Learning multiple layers of features from tiny images},
        author={Krizhevsky, Alex and Hinton, Geoffrey and others},
        year={2009},
        publisher={Citeseer}
    }
    @article{zeng2020adversarial,
        title={Are adversarial examples created equal? A learnable weighted minimax risk for robustness under non-uniform attacks},
        author={Zeng, Huimin and Zhu, Chen and Goldstein, Tom and Huang, Furong},
        journal={arXiv preprint arXiv:2010.12989},
        volume={24},
        year={2020}
    }
    @article{goodfellow2014explaining,
        title={Explaining and harnessing adversarial examples},
        author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
        journal={arXiv preprint arXiv:1412.6572},
        year={2014}
    }
    @article{madry2017towards,
        title={Towards deep learning models resistant to adversarial attacks},
        author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
        journal={arXiv preprint arXiv:1706.06083},
        year={2017}
    }
</script>
</html>

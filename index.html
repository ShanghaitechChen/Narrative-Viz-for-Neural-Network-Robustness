<!DOCTYPE HTML>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="author" content="co-authored by Chen Chen, Ethan Remsberg and Jason Tan">
<title>A Visual Tour to Empirical Neural Network Robustness</title>
<link rel="stylesheet" href="./style.css">
<link rel="stylesheet" href="./distill.css">
<script src="//polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="template.v1.js"></script>
<script src="//d3js.org/d3.v5.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/d3-legend/2.25.6/d3-legend.min.js"></script>
</head>
<body>
    <div class="container">
        <div class="fixed-view">
            <div id="scatterplot" class="scatterplot"></div>
        </div>
        <div class="scrollable-views">
            <d-title>
                <h1>A Visual Tour to Empirical Neural Network Robustness</h1><br>   
            </d-title>
            <d-byline>
                <div class="byline grid">
                <div class="authors-affiliations grid">
                    <h3>Authors</h3>
                    <h3>Affiliations</h3>
                    
                    <p class="author">
                        
                        <a class="name" href="https://ccdtc.cc/">Chen Chen</a>
                    </p>
                    <p class="affiliation">
                    <a class="affiliation" href="https://www.cs.umd.edu/">University of Maryland</a>
                    </p>
                    
                    <p class="author">
                        
                        <a class="name" href="https://github.com/ethan-rems">Ethan Remsberg</a>
                    </p>
                    <p class="affiliation">
                    <a class="affiliation" href="https://www.cs.umd.edu/">University of Maryland</a>
                    </p>
                    
                    <p class="author">
                        
                        <a class="name" href="https://github.com/omivore">Jason Tan</a>
                    </p>
                    <p class="affiliation">
                    <a class="affiliation" href="https://www.cs.umd.edu/">University of Maryland</a>
                    </p>
                    
                </div>
                <div>
                    <h3>Published</h3>
                    
                    <p><em>Not published yet.</em></p>
                </div>
                <div>
                    <h3>DOI</h3>
                    
                    <p><em>No DOI yet.</em></p>
                </div>
                </div>
            </d-byline>
            <h2> Neural Network's Success </h2>
            <p class="text4G"> Neural networks are the most powerful classifiers for image classification tasks at present. To obtain such a classifier, one first needs to define a network architecture \(f(w,x)\)
                parameterized by \(w\) as well as a loss function \(L(w, x, y)\), where \(y\) denotes the real labels, and then perform the gradient descent optimization over the loss function to obtain 
                a set of (approximately) optimal parameters \(w\). To understand gradient descent method for neural networks, you can check <a href="https://machinelearningmastery.com/a-gentle-introduction-to-gradient-descent-procedure/" target="_blank">this tutorial</a> and <a href="https://xnought.github.io/backprop-explainer/" target="_blank">this interactive article</a>. <br>
                In the visualizations below, we show how the loss value and the test accuracy evolve, and how the output probabilities change for 10 example images using a benchmark neural network 
                ResNet-18<dt-cite key="he2016deep"></dt-cite> on the commonly-used dataset CIFAR-10<dt-cite key="krizhevsky2009learning"></dt-cite> (the training details can be found in <dt-cite key="zeng2020adversarial"></dt-cite>) . Select an image, hover over the left-hand-side line chart, and then a racing bar chart 
                will display the changes of the predictive probabilities at different training epochs. You could find some interesting patterns there; for example, the prediction for the <i>Dog</i> 
                image (2nd row, 1st column) jumps between <i>Cat</i> and <i>Dog</i> many times as the training iterates.
            </p>
            <section id="section4G1">
                <div class="div4G" style="text-align:center;">
                    <svg id="G1"></svg>
                </div>
            </section>
            <h2> PGD Attack </h2>
            <p class="text4G"> Consider the situation where an input image is altered by some pixel-level noise that is imperceptible to the human eye. In general, humans won’t be fooled; but unfortunately this is not 
                true for neural networks. In fact, noises found in certain ways are able to mislead a neural network almost always. One of the popular attack methods is the <i>Projected Gradient Descent (PGD) attack</i> <dt-cite key="goodfellow2014explaining,madry2017towards"></dt-cite>. As its name
                indicates, the PGD attack is a gradient-based attacker. It mainly contains the following steps:
                <ol>
                    <li>Start from a random perturbation \(x^{\prime}\) of a given clean image \(x\) such that \(\|x^{\prime} - x\|_p \leq \epsilon\);</li>
                    <li>Update \(x^{\prime}\) by one step of gradient ascent to increase the loss value;</li>
                    <li>Project \(x^{\prime}\) back into the \(\epsilon\)-radius \(L_p\) ball if necessary;</li>
                    <li>Repeat step 2–3 until convergence or reaching a desired number of steps \(ns\).</li>
                </ol>
            </p>
            <p class="text4G">
                Note that the \(\epsilon\)-radius \(L_p\) ball above enforces the condition that "<i>the noise is imperceptible to the human eye</i>". In the visualizations below, we set the hyperparameters
                as \(\{p=+\infty\), \(\epsilon=8/255\), \(ns=10\}\), and display how the neural network's predictions are manipulated and how the adversarial images look at different steps of PGD attack.
                The user can make use of the slider to step through the PGD attack, and can select from the 10 example images by clicking on them. In most cases, the 
                attacker succeeds in no more than 3 steps, while the adversarial images look nearly identical to their original counterparts. Do you have any other interesting findings?
            </p>
            <section id="section4G2">
                <div id="slider" style="height: 100px; width: 400px; position: relative; top: 30px; left: 20%; z-index: 99;">
                    <input id="epoch_slider">
                    <p id="slider_text"></p>
                </div>
                <div id="G2" class="div4G" style="text-align:center;">
                </div>
            </section>
            <h2> Adversarial Training </h2>
            <p class="text4G">
                Here comes a natural question: can we avoid this? The answer is yes! A popular and effective solution is <i>adversarial training</i> <dt-cite key="madry2017towards"></dt-cite>. The idea is to utilize the PGD attack in your 
                training process, that is, train your model with adversarial images instead of clean images at each epoch. Although the idea is simple, the optimization behind it is not trivial. In standard training, we
                solve the following minimization problem:
                $$
                    \min _{\theta} \rho(\theta), \quad \text { where } \quad \rho(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[L(w, x, y)\right],
                $$
                While in adversarial training, we are solving a Min-Max problem:
                $$
                    \min _{\theta} \rho(\theta), \quad \text { where } \quad \rho(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\max _{\delta \in \mathcal{S}} L(w, x+\delta, y)\right],
                $$
                where \(\mathcal{S} = \{x^{\prime}\mid\|x^{\prime} - x\|_p \leq \epsilon\}\) denotes the \(\epsilon\)-radius \(L_p\) ball. In theory, the convergence conditions for such a Min-Max problem over a general
                function is hard to determine; however, in practice, adversarial training converges for most neural networks. In the following visualizations, we compare the output probabilities of a regular-trained model
                and an adversarial-trained model when the same PGD attack is applied, and display how the probabilities flow as the PGD attack iterates. Like above, the user can select from the set of 10 images, and view the difference in performance between the two models.
                It is clear that the adversarial-trained model is more robust. One thing
                worth mentioning here is that the adversarial-trained model cannot resist PGD attack all the time: its accuracy under the PGD attack is \(50.51\%\) (that for the regular-trained model is only \(0.01\%\)).
            </p>
            <section id="section4G3">
                <div id="G3" class="div4G" style="text-align:center;">
                </div>
            </section>
            <h2>
                Accuracy-Robustness Tradeoff
            </h2>
            <p class="text4G">
                There is no free lunch: while adversarial training enhances the model's robustness, it comes at the price of a standard accuracy drop. The adversarial-trained model achieves an \(80.05\%\) accuracy on
                clean test images while that for the regular-trained model is \(91.64\%\). This creates an Accuracy-Robustness tradeoff, and people are now interested in how we can achieve both. The line chart below
                gives a sense about how this tradeoff looks during training.
            </p>
            <section id="section4G4">
                <div id="div4G4" class="div4G" style="text-align:center;">
                    <svg id="G4"></svg>
                </div>
            </section>
            <p class="text4G">
                The Accuracy-Robustness tradeoff is not binary; instead, adversarial training is influenced by the hyperparameter \(\epsilon\). It is expected that the Accuracy-Robustness tradeoff varies when \(\epsilon\) changes. In general,
                a smaller \(\epsilon\) leads to an adversarial-trained model that is more accurate on clean images but less resistant to PGD attack, and vice versa. We adversarial-trained 5 models with \(\epsilon=1/255,\ 2/255,\ 4/255,\ 8/266,\ 16/255\) 
                (other hyperparameters, including the attacker, are the same to make a fair comparison), and present their clean accuracy and adversarial accuracy (i.e., accuracy under PGD attack) in the scatter plot below.
                Note that the model with \(\epsilon=16/255\) doesn't bring a better robustness than that of the model with \(\epsilon=8/255\), thus the rule mentioned above is only valid within a certain range.
            </p>
            <section id="section4G5">
                <div id="div4G5" class="div4G" style="text-align:center;">
                    <svg id="G5"></svg>
                </div>
            </section>
            <d-bibliography src="bibliography.bib"></d-bibliography>
            <h3>
                Citations
            </h3>
            <p>
                [1] He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. <br>
                [2] Krizhevsky, Alex, and Geoffrey Hinton. "Learning multiple layers of features from tiny images." (2009): 7.<br>
                [3] Zeng, Huimin, et al. "Are Adversarial Examples Created Equal? A Learnable Weighted Minimax Risk for Robustness under Non-uniform Attacks." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.<br>
                [4] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. "Explaining and harnessing adversarial examples." arXiv preprint arXiv:1412.6572 (2014).<br>
                [5] Madry, Aleksander, et al. "Towards Deep Learning Models Resistant to Adversarial Attacks." International Conference on Learning Representations. 2018.<br>
            </p>
        </div>
    </div>

    
</body>
<script>
    function wrap(text, width) {
        text.each(function () {
            var text = d3.select(this),
                words = text.text().split(/\s+/).reverse(),
                word,
                line = [],
                lineNumber = 0,
                lineHeight = 1.1, // ems
                x = text.attr("x"),
                y = text.attr("y"),
                dy = 0, //parseFloat(text.attr("dy")),
                tspan = text.text(null)
                            .append("tspan")
                            .attr("x", x)
                            .attr("y", y)
                            .attr("dy", dy + "em");
            while (word = words.pop()) {
                line.push(word);
                tspan.text(line.join(" "));
                if (tspan.node().getComputedTextLength() > width) {
                    line.pop();
                    tspan.text(line.join(" "));
                    line = [word];
                    tspan = text.append("tspan")
                                .attr("x", x)
                                .attr("y", y)
                                .attr("dy", ++lineNumber * lineHeight + dy + "em")
                                .text(word);
                }
            }
        });
    }
    var chosenImg = 0;
    // D3 script to load JSON and render scatter plot
    d3.json('./Datasets/_projections.json').then(data => {
        const margin = { top: 20, right: 20, bottom: 20, left: 20 };
        const width = document.getElementById('scatterplot').clientWidth - margin.left - margin.right;
        const height = document.getElementById('scatterplot').clientHeight - margin.top - margin.bottom;

        const svg = d3.select("#scatterplot")
            .append("svg")
            .attr("width", "100%")
            .attr("height", "100%")
            .append("g")
            .attr("transform", `translate(${margin.left},${margin.top})`);

        const xExtent = d3.extent(data, d => d.position[0]);
        const yExtent = d3.extent(data, d => d.position[1]);

        const xScale = d3.scaleLinear()
            .domain(xExtent)
            .range([0, width]);

        const yScale = d3.scaleLinear()
            .domain(yExtent)
            .range([height, 0]);

        const topRightImage = svg.append("image")
            .attr("x", width - 110) // Adjust position to top-right corner
            .attr("y", 10) // Adjust position to top-right corner
            .attr("width", 100) // Larger size
            .attr("height", 100) // Larger size
            .attr("border", "#b41930")
            .attr("class", "top-right-image")
            .attr("xlink:href", `./images/image_${data[158].key}.png`);

        svg.selectAll("image")
            .data(data)
            .enter()
            .append("image")
            .attr("x", d => xScale(d.position[0]) - 12) // Adjust for image size
            .attr("y", d => yScale(d.position[1]) - 12) // Adjust for image size
            .attr("width", 24)
            .attr("height", 24)
            .attr("xlink:href", d => `./images/image_${d.key}.png`)
            .attr("class", "scatterplot-image")
            .attr("id", d => `scatterplot-image-${d.key}`)
            .on("click", d => {
                chosenImg = parseInt(d.key);
                topRightImage.attr("xlink:href", `./images/image_${d.key}.png`);
                // d3.selectAll(".scatterplot-image").classed("g1-selected-image", false);
                // d3.select("#scatterplot-image-" + d.key).classed("g1-selected-image", true);

                svg.selectAll("rect").remove();
                // Append a new highlight rectangle
                svg.append("rect")
                    .attr("x", xScale(d.position[0]) - 14) // Adjust position to be on the border
                    .attr("y", yScale(d.position[1]) - 14) // Adjust position to be on the border
                    .attr("width", 28) // Rectangle size
                    .attr("height", 28) // Rectangle size
                    .attr("fill", "none")
                    .attr("stroke", "red")
                    .attr("stroke-width", 2);
                updateHeat();
            });

        svg.append("rect")
            .attr("x", xScale(data[158].position[0]) - 14) // Adjust position to be on the border
            .attr("y", yScale(data[158].position[1]) - 14) // Adjust position to be on the border
            .attr("width", 28) // Rectangle size
            .attr("height", 28) // Rectangle size
            .attr("fill", "none")
            .attr("stroke", "red")
            .attr("stroke-width", 2);
    }).catch(error => {
        console.error('Error loading JSON file:', error);
    });
</script>
<script src="./d3-tip.js"></script>
<script src="./G1/main.js"></script>
<script src="./G2/main.js"></script>
<script src="./G3/main.js"></script>
<script src="./G4/main.js"></script>
<script src="./G5/main.js"></script>
<script src="http://yui.yahooapis.com/3.18.1/build/yui/yui-min.js"></script>

<script type="text/bibliography">
    @inproceedings{he2016deep,
        title={Deep residual learning for image recognition},
        author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
        booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
        pages={770--778},
        year={2016}
      }
    @article{krizhevsky2009learning,
        title={Learning multiple layers of features from tiny images},
        author={Krizhevsky, Alex and Hinton, Geoffrey and others},
        year={2009},
        publisher={Citeseer}
    }
    @article{zeng2020adversarial,
        title={Are adversarial examples created equal? A learnable weighted minimax risk for robustness under non-uniform attacks},
        author={Zeng, Huimin and Zhu, Chen and Goldstein, Tom and Huang, Furong},
        journal={arXiv preprint arXiv:2010.12989},
        volume={24},
        year={2020}
    }
    @article{goodfellow2014explaining,
        title={Explaining and harnessing adversarial examples},
        author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
        journal={arXiv preprint arXiv:1412.6572},
        year={2014}
    }
    @article{madry2017towards,
        title={Towards deep learning models resistant to adversarial attacks},
        author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
        journal={arXiv preprint arXiv:1706.06083},
        year={2017}
    }
</script>
</html>
